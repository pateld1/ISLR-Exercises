---
title: 'MLStats: Linear Model Selection and Regularization'
author: "Darshan Patel"
date: "2/12/2019"
output: 
  md_document:
    variant: markdown_github
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

In this assignment, mimic the lab exercises from ISLR Chapter 6: Linear Model Selection and Regularization

## Libraries
Load the following libraries.
```{r, message=FALSE}
rm(list = ls())
library(tidyverse)
library(scales)
library(gridExtra)
library(RColorBrewer)
library(leaps)
library(glmnet)
library(pls)
```

## Dataset
In this assignment, the dataset that will be used is `student-mat.csv`. It is a collection of attributes on student achievement in secondary math education of two Portuguese schools. 

(Source: https://archive.ics.uci.edu/ml/datasets/Student+Performance)

This data approach student achievement in secondary education of two Portuguese schools. The data attributes include student grades, demographic, social and school related features) and it was collected by using school reports and questionnaires. Two datasets are provided regarding the performance in two distinct subjects: Mathematics (mat) and Portuguese language (por). In [Cortez and Silva, 2008], the two datasets were modeled under binary/five-level classification and regression tasks. Important note: the target attribute G3 has a strong correlation with attributes G2 and G1. This occurs because G3 is the final year grade (issued at the 3rd period), while G1 and G2 correspond to the 1st and 2nd period grades. It is more difficult to predict G3 without G2 and G1, but such prediction is much more useful (see paper source for more details).

```{r}
df = read_delim("student-mat.csv", delim = ";")
```

The size of the dataset is
```{r}
nrow(df)
```

The number of variables in the dataset is
```{r}
ncol(df)
```

The variables are listed below:
```{r}
colnames(df)
```

According to the data description, `G1` and `G2` are first and second period grades while `G3` is the third and final period grade. In this assignment, `G3` will be predicted using all variables including `G1` and `G2`. 

The distribution of the students' final period math grade is shown below.
```{r}
ggplot(df, aes(x = G3)) + geom_histogram(binwidth = 1, 
                                         fill = "deepskyblue", 
                                         color = "deepskyblue4") + 
  ggtitle("Distribution of Students' Final Period Math Grade") + 
  labs(x = "G3 grade", y = "number of students") + 
  theme_minimal()
```

The distribution of students' final math grades is skewed right. Furthermore, there is an arbitrary large number of $0$s received by students. 

With so many variables, it is impossible to look at all variables to see visually which ones more or less correlate with `G3`. Therefore use different techniques to find the best combinations of variables to predict `G3`. 

Just to check, are there null/missing values?
```{r}
any(is.na(df))
```
No nulls. 

## Best Subset Selection

Fit a model using best subset selection. Since best subset selection creates $2^p$ models, and $p = 32$ here, that is a large number of models to create. Therefore, set a max of $8$ variables to use. 
```{r}
model_bss = regsubsets(G3~., df, nvmax = 8, really.big = TRUE)
```

Plot the model statistics as a function of number of variables added to the model.
```{r}
model_bss_stats = data.frame("num_vars" = seq(1,8), 
                             "C_p" = summary(model_bss)$cp,
                            "BIC" = summary(model_bss)$bic, 
                            "adjusted R^2" = summary(model_bss)$adjr2)

g1 = ggplot(model_bss_stats, aes(x = num_vars, y = C_p)) + geom_path() + 
  scale_x_continuous(breaks = seq(1, 8, by = 1)) + 
  labs(x = "number of variable") + 
  ggtitle("C_p as a Function of Number of Variables in Best Subset Selection") + 
  theme_minimal() 
g2 = ggplot(model_bss_stats, aes(x = num_vars, y = BIC)) + geom_path() + 
  scale_x_continuous(breaks = seq(1, 8, by = 1)) + 
  labs(x = "number of variable") + 
  ggtitle("BIC as a Function of Number of Variables in Best Subset Selection") + 
  theme_minimal()
g3 = ggplot(model_bss_stats, aes(x = num_vars, y = adjusted.R.2)) + geom_path() + 
  scale_x_continuous(breaks = seq(1, 8, by = 1)) + 
  labs(x = "number of variable") +
  ggtitle("Adjusted R^2 as a Function of Number of Variables in Best Subset Selection") + 
  theme_minimal()

grid.arrange(g1,g2,g3,ncol=1)
```

Best subset selection does not do well for modeling in this scenario. The $R^2$ value is decent too. The best number of variables to use is
```{r}
min(which.min(summary(model_bss)$cp), 
    which.min(summary(model_bss)$bic))
```

Only $5$ variables are needed. The coefficients of the model are
```{r}
coef(model_bss, 5)
```
Now try using forward and backward stepwise selection.

## Foward and Backward Stepwise Selection 

Perform forward stepwise selection first and report the model statistics. This method is not computationally expensive therefore all variables sizes will be considered.
```{r}
model_fss = regsubsets(G3~., df, nv = 33, method = "forward")

model_fss_stats = data.frame("num_vars" = seq(1,33), 
                             "C_p" = summary(model_fss)$cp,
                            "BIC" = summary(model_fss)$bic, 
                            "adjusted R^2" = summary(model_fss)$adjr2)

g4 = ggplot(model_fss_stats, aes(x = num_vars, y = C_p)) + geom_path() + 
  scale_x_continuous(breaks = seq(1, 33, by = 1)) + 
  labs(x = "number of variable") + 
  ggtitle("C_p as a Function of Number of Variables in Forward Stepwise Selection") + 
  theme_minimal() 
g5 = ggplot(model_fss_stats, aes(x = num_vars, y = BIC)) + geom_path() + 
  scale_x_continuous(breaks = seq(1, 33, by = 1)) + 
  labs(x = "number of variable") + 
  ggtitle("BIC as a Function of Number of Variables in Forward Stepwise Selection") + 
  theme_minimal()
g6 = ggplot(model_fss_stats, aes(x = num_vars, y = adjusted.R.2)) + geom_path() + 
  scale_x_continuous(breaks = seq(1, 33, by = 1)) + 
  labs(x = "number of variable") +
  ggtitle("Adjusted R^2 as a Function of Number of Variables in Forward Stepwise Selection") + 
  theme_minimal()

grid.arrange(g4,g5,g6,ncol=1)
```

Nice low adjusted $R^2$ value. The best number of variables to use is
```{r}
min(which.min(summary(model_fss)$cp), 
    which.min(summary(model_fss)$bic))
```
The best model uses $5$ variables as oppose to $33$ variables. This is the same result as best subset selection. But do the coefficients and specific variables match? The coefficients of the model are
```{r}
coef(model_fss, 5)
```
Best subset selection and forward stepwise selection created the same model.

Now try backward stepwise selection and report the model statistics.
```{r}
model_bwss = regsubsets(G3~., df, nv = 33, method = "backward")

model_bwss_stats = data.frame("num_vars" = seq(1,33), 
                             "C_p" = summary(model_bwss)$cp,
                            "BIC" = summary(model_bwss)$bic, 
                            "adjusted R^2" = summary(model_bwss)$adjr2)

g7 = ggplot(model_bwss_stats, aes(x = num_vars, y = C_p)) + geom_path() + 
  scale_x_continuous(breaks = seq(1, 33, by = 1)) + 
  labs(x = "number of variable") + 
  ggtitle("C_p as a Function of Number of Variables in Backward Stepwise Selection") + 
  theme_minimal() 
g8 = ggplot(model_bwss_stats, aes(x = num_vars, y = BIC)) + geom_path() + 
  scale_x_continuous(breaks = seq(1, 33, by = 1)) + 
  labs(x = "number of variable") + 
  ggtitle("BIC as a Function of Number of Variables in Backward Stepwise Selection") + 
  theme_minimal()
g9 = ggplot(model_bwss_stats, aes(x = num_vars, y = adjusted.R.2)) + geom_path() + 
  scale_x_continuous(breaks = seq(1, 33, by = 1)) + 
  labs(x = "number of variable") +
  ggtitle("Adjusted R^2 as a Function of Number of Variables in Backward Stepwise Selection") + 
  theme_minimal()

grid.arrange(g4,g5,g6,ncol=1)
```

The best number of variables to use is
```{r}
min(which.min(summary(model_bwss)$cp), 
    which.min(summary(model_bwss)$bic))
```
The best model uses $5$ variables. The coefficients of the model are
```{r}
coef(model_fss, 5)
```
The same coefficients and coefficient estimates are found again.

All of the above models appear to be doing ok looking at the adjusted $R^2$ statistic. Try to improve on it using the validation set approach and cross validation and best subset selection.
```{r}
set.seed(25)
indices = sample(1:nrow(df), size = 0.7*nrow(df))
train = df[indices,]
test = df[indices,]
model_bss_vs = regsubsets(G3~., train, nvmax = 33)
test_matrix = model.matrix(G3~., test)
errors_bss = c()

for(i in 1:33){
  c = coef(model_bss_vs, id = i)
  pred = test_matrix[, names(c)] %*% c
  errors_bss = c(errors_bss, mean((test$G3 - pred)^2))
}

error_vs_df = data.frame("num_vars" = seq(1,33, by=1), 
                         "error_bss" = errors_bss)

ggplot(error_vs_df, aes(x = num_vars, y = error_bss)) + geom_path() + 
  ggtitle("Validation Set Error As a Function of Number of Variables in the Model") + 
  labs(x = "number of variables", y = "test set error") + 
  theme_minimal()
```

As opposed to running best subset selection on the entire dataset, running it and testing on a smaller data set resulted in the best number of variables to be $33$. When the entire dataset is used, it is $5$. However creating the model using a split will result in better model performance for the future even if all variables seem to be needed. Note that the test set error do not change as much after $25$ variables. Use this value.

The coefficients of the $25$ variable model are
```{r}
coef(model_bss_vs, 25)
```

Can it be improved using cross-validation? (Note: A smaller `nvmax` parameters was passed since the algorithm was time-consuming.)
```{r}
k = 5
set.seed(25)
folds = sample(1:k, nrow(df), replace = TRUE)
cv_errors = matrix(NA, k, 10, dimnames = list(NULL, paste(1:10)))
predict.regsubsets = function(object, newdata, id, ...){
  form = as.formula(object$call[[2]])
  mat = model.matrix(form, newdata)
  coefi = coef(object, id = id)
  xvars = names(coefi)
  return(mat[,xvars] %*% coefi)
}
for(i in 1:k){
  train = df[folds !=i ,]
  test = df[folds == i ,]
  temp_model = regsubsets(G3~., data = train, nvmax = 10)
  for(j in 1:10){
    pred = predict(temp_model, test, id = j)
    cv_errors[i,j] = mean((test$G3 - pred)^2)
  }
}
mean_cv_errors = apply(cv_errors, 2, mean)
mean_cv_df = data.frame("num_vars" = seq(1,10,by=1),
                        "cv_error" = mean_cv_errors)

ggplot(mean_cv_df, aes(x = num_vars, y = cv_error)) + geom_path() + 
  ggtitle("Mean Cross Validation Error as a Function of Number of Variables") + 
  labs(x = "number of variable", y = "averaged cv error") + 
  scale_x_continuous(breaks = 1:10, labels = 1:10) + 
  theme_minimal()
```

By using cross validation, the "best" model appears to be one that has $5$ variables. 

Using this information, perform best subset selection on the full data set in order to find the $5$-variable model.
```{r}
model_best_cv = regsubsets(G3~., df, nvmax = 5)
coef(model_best_cv, 5)
```

These coefficients have appeared before. 

All of the selection methods have performed similarly on the same train/test split, giving the same give coefficients and coefficient estimates. Using forward/backward stepwise selection was preferred over best subset selection due to its lower computational cost.

Let's see if new estimates can be found via another method.

## Ridge Regression

Fit a ridge regression model on a train/test split to predict `G3`.
```{r}
set.seed(25)
x = model.matrix(G3~., df)[,-1]
y = df$G3
train = sample(1:nrow(x), size = 0.7*nrow(x))
test = setdiff(x, train)

model_rr = glmnet(x[train,], y[train], 
                  alpha = 0, 
                  lambda = 10^seq(10, -5, length = 1000), 
                  thresh = 1e-12)
```

Predict the `G3` scores for when $\lambda = 3$ in the ridge regression model and calculate its test set MSE.
```{r}
pred = predict(model_rr, s = 3, newx = x[test,])
mean((pred - y[test])^2)
```

Instead of arbitrarily picking a $\lambda$ value, use cross-validation to pick the optimal value. 
```{r}
set.seed(25)
cv_model_rr = cv.glmnet(x[train,], y[train], alpha = 0)
```

The following plot shows how cross-validation error changes as $\lambda$ increases.
```{r}
cv_rr_df = data.frame("lambda" = cv_model_rr$lambda, "mse" = cv_model_rr$cvm)
ggplot(cv_rr_df, aes(x = lambda, y = mse)) + geom_path() +
  ggtitle("Cross-Validated Error as a Function of Lambda") + 
  scale_x_continuous(limits = c(0.4, 1), breaks = c(0.4,1)) + 
  scale_y_continuous(limits = c(4,5), breaks = c(4,5))
```

The optimal $\lambda$ value is
```{r}
cv_model_rr$lambda.min
```

Using this value, predict on the test set. 
```{r}
pred = predict(model_rr, s = cv_model_rr$lambda.min, newx = x[test,])
mean((pred - y[test])^2)
```
This is a lower test set MSE than the one received by randomly choosing a $\lambda$ value. 

Now refit the ridge regression model on the full data set and predict using the value of $\lambda$ above to attain the coefficient estimates.
```{r}
model_rr_best = glmnet(x, y, alpha = 0)
predict(model_rr_best, type = "coefficients", s = cv_model_rr$lambda.min)
```
Note that some coefficient estimates are close to $0$ but none are actually $0$. This shows that ridge regression does not perform variable selection.

Now lasso.

## Lasso Regression

Fit a lasso model on the same train/test split as above and perform cross validation to find the optimal $\lambda$ value.
```{r}
set.seed(25)
model_lasso = glmnet(x[train,], y[train], alpha = 1, lambda = 10^seq(10,-5, length = 1000))
cv_model_lasso = cv.glmnet(x[train,], y[train], alpha = 1)
```

The following plot shows how cross-validation error changes as $\lambda$ increases.
```{r}
cv_lasso_df = data.frame("lambda" = cv_model_lasso$lambda, "mse" = cv_model_lasso$cvm)
ggplot(cv_lasso_df, aes(x = lambda, y = mse)) + geom_path() +
  ggtitle("Cross-Validated Error as a Function of Lambda") + 
  scale_x_continuous(limits = c(0, 1), breaks = c(0,1)) + 
  scale_y_continuous(limits = c(2.5,5), breaks = c(2.5,5))
```

The optimal $\lambda$ value is 
```{r}
cv_model_lasso$lambda.min
```

That is really close to $0$.. The $\lambda$ value obtained from ridge regression was slightly larger. Goes to show how specifiying which norm to use in the regression equation will impact the model. 

Using this value, predict on the test set.
```{r}
pred = predict(model_lasso, s = cv_model_lasso$lambda.min, newx = x[test,])
mean((pred - y[test])^2)
```

This is a lower test set MSE than the one obtained by ridge regression! 

Now refit the lasso regression model on the full data set and predict using the value of $\lambda$ above to attain the coefficient estimates.
```{r}
model_lasso_best = glmnet(x, y, alpha = 1)
predict(model_lasso_best, type = "coefficients", s = cv_model_lasso$lambda.min)
```
Just as expected! The lasso regression model allows some coefficient estimates to be zero and it did as so. $29$ of the $41$ variables have been effectively wiped out. This is better than what subset selection gave. Hence the lasso model with $\lambda$ chosen by cross validation contains only $12$ variables. 

But there's two more techniques to consider.

## Principal Components Regression

Perform PCR on the entire dataset to predict `G3` grades.
```{r}
set.seed(25)
model_pcr = pcr(G3 ~ ., data = df, scale = TRUE, validation = "CV")
```

The cross validation score for each possible number of components is plotted below.
```{r}
validationplot(model_pcr, val.type = "MSEP")
```

The smallest cross-validation error occurs when $M = 40$. 

Now perform PCR on the same train/test split as before. Plot the cross-validation errors.
```{r}
set.seed(25)
cv_model_pcr = pcr(G3~., data = df[train,], scale = TRUE, validation = "CV")
validationplot(cv_model_pcr, val.type = "MSEP")
```

The lowest cross-validation occur occurs when $M = 39$. Compute the test set MSE using this $M$ value.
```{r}
pred = predict(cv_model_pcr, x[test,], ncomp = 39)
mean((pred - y[test])^2)
```

This model has performed slightly worse than the ridge and lasso regression. 

Now fit PCR on the full data set using $M=39$. 
```{r}
model_pcr_full = pcr(y~x, scale = TRUE, ncomp = 39)
summary(model_pcr_full)
```
Using $39$ components, $99.56\%$ of the variation in the predictors and $84.14\%$ of the variation in the response is explained by the $39$ components. Note that the PCR does not try to explain the variance in the response. That's what the next method will do. 

Last method, PLS (no pun intended). 

## Partial Least Squares

Fit a PLS model on training set and plot the cross-validation error.
```{r}
cv_model_pls = plsr(G3~., data = df[train,], scale = TRUE, validation = "CV")
validationplot(cv_model_pls, val.type = "MSEP")
```

The lowest cross-validation error occurs when.. 
```{r}
summary(cv_model_pls)
```
When $M=10$! $10$ partial least squares direction is needed to explain the variation in the response and predictors.

Using this $M$ value, predict on the test set and report the MSE.
```{r}
pred = predict(cv_model_pls, x[test,], ncomp = 10)
mean((pred - y[test])^2)
```
The test set error is lower here than the one obtained by PCR.

Perform PLS on the entire data set using $M=10$. 
```{r}
model_pls_full = plsr(y~x, data = df, scale = TRUE, ncomp = 10)
summary(model_pls_full)
```
Note that with PLS, $84.57\%$ of the variance in the final math grade (and $37.89\%$ of the variance in the predictors) is explained by $10$ components. Contrasting with that, $84.14\%$ of the variance in the final math grade was explained by $39$ components in the PCR model. The PLS outperformed the PCR here in explaining the variance in the final math grade. This came at a cost of a huge decrease in the percent of variance explained explained in the predictors. 


All of the lab instructions in this document are taken from "An Introduction to Statistical Learning, with applications in R"  (Springer, 2013) with permission from the authors: G. James, D. Witten,  T. Hastie and R. Tibshirani. 